version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      MODEL_ID: "${MODEL_ID:-qwen2.5-coder:14b}"
    entrypoint: ["/bin/sh","-lc","ollama serve & sleep 4 && if [ -n \"$MODEL_ID\" ]; then ollama pull \"$MODEL_ID\" || true; fi && tail -f /dev/null"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sSf http://localhost:11434/api/tags | grep -q \"${MODEL_ID:-qwen2.5-coder:14b}\" || exit 1"]
      interval: 10s
      timeout: 20s
      retries: 60
      start_period: 10s
    deploy:
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          memory: 12G
        limits:
          memory: 14G
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

  code-agent:
    build:
      context: ./
      dockerfile: Dockerfile
    image: local-code-writer-agent:latest
    container_name: code-writer-agent
    depends_on:
      - ollama
    environment:
      OLLAMA_BASE_URL: "http://ollama:11434"
      MODEL_ID: "${MODEL_ID:-qwen2.5-coder:14b}"
      MAX_TOKENS: "4096"
      TEMPERATURE: "0.2"
    ports:
      - "8090:8090"
    volumes:
      - ./out:/app/out
    deploy:
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          memory: 512M
        limits:
          memory: 1G
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

volumes:
  ollama:
